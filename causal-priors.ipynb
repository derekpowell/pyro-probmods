{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "committed-accordance",
   "metadata": {},
   "source": [
    "# Inferring priors for causal learning\n",
    "\n",
    "Researchers studying causal learning most commonly examine how people learn causal relationships from observing covarations among events. This has borne a lot of fruit and touched on some deep philosophical issues about the nature of causality (e.g., it’s all in our heads!). However, little of the causal knowledge that most people actually possess is achieved in this way. Instead, most of our causal knowledge comes from what others tell us about the world. Of course, people rarely convey causal information like, “the causal power of aspirin to reduce headaches is .25.” Instead, we communicate more vague information, like “aspirin helps with headaches.” Moreover, we communicate and possess even more abstract understandings of how different types of causal entities interact and how different different causal mechanisms operate, that let us draw inferences about specific causal relationships from generalities. How do these forms of knowledge interact in our causal inferences?\n",
    "\n",
    "This is a preliminary exploration for a project I’m considering that would examine how mechanistic understandings or explanations might shape people’s priors about causal relationships. The idea is that our abstract understanding of causal mechanisms could be expressed as different priors about the possible strengths of causal relationships. Given different cover stories suggesting different potential mechanisms, we should expect different kinds of observations. \n",
    "\n",
    "With that, the idea is to use numpyro to combine __bayesian cognitive modeling and bayesian data analysis__ to infer causal learner’s domain-specific priors based on their responses in a causal learning task. This would begin to suggest ways in which we might connect these two forms of causal knowledge.\n",
    "\n",
    "This notebook is a preliminary proof-of-concept test to see whether this kind of approach could work. In this notebook, I’ll simulate some causal power estimates with a model of a causal learner. I’ll simulate data for two different sets of contingency data and with priors for two different contexts. Then, I’ll do bayesian data analysis over the bayesian cognitive model and we’ll see how well we can recover the original parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "rapid-article",
   "metadata": {},
   "outputs": [],
   "source": [
    "## to use SA without gradients\n",
    "# !pip uninstall numpyro jax jaxlib\n",
    "# !pip install git+https://github.com/pyro-ppl/numpyro.git@ca4763f1d3cbabc6e31a17919e3e6697b3a1568e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "diagnostic-scheduling",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpyro\n",
    "numpyro.enable_x64()\n",
    "\n",
    "import numpyro.distributions as dist\n",
    "from numpyro import handlers\n",
    "from numpyro.infer import  MCMC, SA, Predictive, NUTS, DiscreteHMCGibbs, SVI, Trace_ELBO\n",
    "from numpyro.infer.autoguide import AutoLaplaceApproximation, AutoMultivariateNormal\n",
    "\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from jax.random import PRNGKey\n",
    "from jax.scipy.special import expit, logit\n",
    "\n",
    "from tensorflow_probability import distributions as tfdist\n",
    "\n",
    "import seaborn as sns\n",
    "import functools\n",
    "import pandas as pd\n",
    "\n",
    "def make_random(fn):\n",
    "    key = jax.random.PRNGKey(np.random.randint(0, 1e6))\n",
    "    return handlers.seed(fn, rng_seed=key)\n",
    "\n",
    "def repeat_func(n, fn, *args):\n",
    "    return jnp.stack([fn(*args) for _ in range(0,n)])\n",
    "\n",
    "def laplace_approx(model, *args):\n",
    "    guide = AutoLaplaceApproximation(model)\n",
    "    optimizer = numpyro.optim.Minimize()\n",
    "    svi = SVI(model, guide, optimizer, Trace_ELBO())\n",
    "    init_state = svi.init(PRNGKey(0), *args)\n",
    "    optimal_state, loss = svi.update(init_state, *args)\n",
    "    return guide.get_posterior(svi.get_params(optimal_state))\n",
    "\n",
    "def MVNorm_approx(model, *args):\n",
    "    guide = AutoMultivariateNormal(model)\n",
    "    optimizer = numpyro.optim.Adagrad(step_size=.1)\n",
    "    svi = SVI(model, guide, optimizer, Trace_ELBO())\n",
    "    result = svi.run(PRNGKey(0), 3000, *args, progress_bar=False)\n",
    "    transform = guide.get_transform(result.params)\n",
    "    raw_posterior = guide.get_posterior(result.params)\n",
    "    \n",
    "    return raw_posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generic-elizabeth",
   "metadata": {},
   "source": [
    "# Simulate Responses\n",
    "\n",
    "As an example, here I sample the posterior over expected occurrences of some effect E, given cause C and background causes B, after learning the contingency data where the effect occurs 2/10 times when the cause is not present, and 9/10 times when the cause is present.\n",
    "\n",
    "I'll use MCMC to sample from the causal learner's posterior, first with a uniform prior to see the \"true\" (unbiased) causal powers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "congressional-orleans",
   "metadata": {},
   "outputs": [],
   "source": [
    "contingencies = {\n",
    "    \"C\":jnp.array([0,1]), \n",
    "    \"effect\":jnp.array([2,9]), \n",
    "    \"N\":jnp.array([10,10])\n",
    "}\n",
    "\n",
    "def powerPCprob(c, wc, wb):\n",
    "    # c: 0 or 1    # wc: [0,1]    # wB: [0,1]\n",
    "    return c*wc + wb - c*wc*wb\n",
    "\n",
    "prior_uniform = (1,1)\n",
    "prior_weak = (2,5)\n",
    "prior_strong = (5,2)\n",
    "\n",
    "def observerModel(prior_tuple, contingencies):\n",
    "    # priors = distribution object\n",
    "    # contingencies = dict\n",
    "    cause = contingencies[\"C\"]\n",
    "    effect = contingencies[\"effect\"]\n",
    "    N = contingencies[\"N\"]\n",
    "\n",
    "    wc = numpyro.sample(\"wc\", dist.Beta(*prior_tuple))\n",
    "    wb = numpyro.sample(\"wb\", dist.Beta(1,1))\n",
    "    prob = powerPCprob(cause, wc, wb)\n",
    "    with numpyro.plate(\"obs\", len(cause)):\n",
    "        numpyro.sample(\"cmodel\", dist.Binomial(N, prob), obs=effect)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "alpha-realtor",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 1000/1000 [00:02<00:00, 411.34it/s, 3 steps of size 9.14e-01. acc. prob=0.86]\n"
     ]
    }
   ],
   "source": [
    "kernel = NUTS(observerModel, target_accept_prob=.80)\n",
    "posterior = MCMC(kernel, 500, 500, num_chains=1)\n",
    "\n",
    "posterior.run(PRNGKey(0), prior_uniform, contingencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "straight-today",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                mean       std    median      5.0%     95.0%     n_eff     r_hat\n",
      "        wb      0.28      0.13      0.26      0.06      0.47    426.62      1.00\n",
      "        wc      0.76      0.15      0.79      0.55      1.00    327.98      1.00\n",
      "\n",
      "Number of divergences: 0\n"
     ]
    }
   ],
   "source": [
    "posterior.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grand-calibration",
   "metadata": {},
   "source": [
    "And we get the same results by laplace approximation for this simple model (modulo sampling error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "smoking-authentication",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([0.26596262, 0.78139321], dtype=float64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expit(laplace_approx(observerModel, prior_uniform, contingencies).loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-philip",
   "metadata": {},
   "source": [
    "I'll fit the model twice for strong and weak priors on $w_c$ and use the posterior to simulate participants responses.\n",
    "\n",
    "For weak priors, I'll assume $w_c \\sim beta(2,5)$ and for strong priors I'll assume  $w_c \\sim beta(5,2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "technological-traveler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                mean       std    median      5.0%     95.0%     n_eff     r_hat\n",
      "        wb      0.34      0.14      0.33      0.10      0.54    229.69      1.00\n",
      "        wc      0.51      0.17      0.51      0.26      0.80    191.89      1.00\n",
      "\n",
      "Number of divergences: 0\n"
     ]
    }
   ],
   "source": [
    "posterior_weak = MCMC(NUTS(observerModel, target_accept_prob=.80),\n",
    "                      500, 500, num_chains=1, progress_bar=False)\n",
    "posterior_weak.run(PRNGKey(0), prior_weak, contingencies)\n",
    "\n",
    "posterior_weak.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "square-instrument",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                mean       std    median      5.0%     95.0%     n_eff     r_hat\n",
      "        wb      0.27      0.12      0.25      0.09      0.46    437.35      1.00\n",
      "        wc      0.78      0.12      0.79      0.60      0.95    237.26      1.00\n",
      "\n",
      "Number of divergences: 0\n"
     ]
    }
   ],
   "source": [
    "posterior_strong = MCMC(\n",
    "    NUTS(observerModel, target_accept_prob=.80), \n",
    "    500, 500, num_chains=1, progress_bar=False)\n",
    "posterior_strong.run(PRNGKey(0), prior_strong, contingencies)\n",
    "\n",
    "posterior_strong.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reasonable-crazy",
   "metadata": {},
   "source": [
    "Below are some plots of the posteriors on $w_c$ for strong and weak priors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "comparative-saying",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Count'>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "c_resp_weak = posterior_weak.get_samples()[\"wc\"]\n",
    "b_resp_weak = posterior_weak.get_samples()[\"wb\"]\n",
    "\n",
    "c_resp_strong = posterior_strong.get_samples()[\"wc\"]\n",
    "b_resp_strong = posterior_strong.get_samples()[\"wb\"]\n",
    "\n",
    "sns.histplot(c_resp_strong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "attached-savannah",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Count'>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(c_resp_weak)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excellent-cocktail",
   "metadata": {},
   "source": [
    "# Inferring priors from responses\n",
    "\n",
    "Now we’ll do bayesian data analysis over our bayesian cognitive model, and see if we can recover the parameters of the prior distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bacterial-soviet",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\"c_resp\": c_resp_weak[-100:], \"b_resp\": b_resp_weak[-100:]}\n",
    "\n",
    "def model(responses, contingencies):\n",
    "    c_resp = responses[\"c_resp\"]\n",
    "    b_resp = responses[\"b_resp\"]\n",
    "    \n",
    "    a = numpyro.sample(\"a\", dist.HalfCauchy(10))\n",
    "    b = numpyro.sample(\"b\", dist.HalfCauchy(10))\n",
    "    \n",
    "    with handlers.block():\n",
    "        pred_powers = laplace_approx(observerModel, (a, b), contingencies)\n",
    "    \n",
    "    resps = jnp.stack([b_resp, c_resp], axis=-1)\n",
    "    \n",
    "    with numpyro.plate(\"data\", c_resp.shape[0]):\n",
    "        numpyro.sample(\"obs\", pred_powers, obs=logit(resps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "armed-malpractice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.6 s, sys: 135 ms, total: 23.8 s\n",
      "Wall time: 23.6 s\n"
     ]
    }
   ],
   "source": [
    "# posterior = MCMC(NUTS(model), 1_000, 1_000, num_chains=1)\n",
    "posterior = MCMC(SA(model), 40_000, 40_000, num_chains=1, progress_bar=False)\n",
    "%time posterior.run(PRNGKey(0), data_dict, contingencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "future-injury",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                mean       std    median      5.0%     95.0%     n_eff     r_hat\n",
      "         a      1.36      0.47      1.35      0.53      2.08    427.16      1.01\n",
      "         b      4.86      0.58      4.84      3.86      5.78    578.44      1.01\n",
      "\n",
      "Number of divergences: 0\n"
     ]
    }
   ],
   "source": [
    "posterior.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "involved-specific",
   "metadata": {},
   "source": [
    "# Voila!\n",
    "\n",
    "Seems like this is sort of approximately working! I may need to simulate some better/more diagnostic data. And the effective samples are pretty low, but no divergent transitions. Some notes:\n",
    "\n",
    "* Works with my `MVNorm_approx()` and `NUTS` MCMC\n",
    "* Also works with `laplace_approx()` and `SA` MCMC, but gives more biased results (but it's super fast!)\n",
    "* Runs with `laplace_approx()` and `NUTS(..., forward_mode_differentiation=True)` but gives totally messed up results.\n",
    "\n",
    "This could be extended to some of the \"everyday reasoning\" work where instead of just running forward predictions and comparing medians to \"correct\" values, we could instead infer the shape of the prior in people's heads (if we could assume it has some parametric form). It would be a kind of cool replication/extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-draft",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
